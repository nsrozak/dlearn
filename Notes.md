Notes
*ReLU Activation Function*
- Cheap to compute
- Gradients do not vanish and there is no saturation when X > 0
*Dropout Regularization*
- Teaches the network to not become too dependent on certain features